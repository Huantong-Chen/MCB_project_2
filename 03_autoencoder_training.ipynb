{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "lMElGYUIljFH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tensorflow.python.keras.layers import Input, Dense\n",
    "from tensorflow.python.keras.layers import LeakyReLU\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2mpESlIpJDrV"
   },
   "outputs": [],
   "source": [
    "# Autoencoder based on https://dropsofai.com/autoencoders-in-keras-and-deep-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Fe_5CbDzbT-x"
   },
   "outputs": [],
   "source": [
    "def unison_shuffled_copies(a, b):\n",
    "    # https://stackoverflow.com/questions/4601373/better-way-to-shuffle-two-numpy-arrays-in-unison\n",
    "    # mix two arrays randomly in parallel\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "id": "YUoY4bN4mfex",
    "outputId": "ef94f34b-f2de-4837-ee10-f51fc9bf44d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wildtype Training Data Shape: (40000, 101)\n",
      "Wildtype Label Data Shape:    (40000,)\n",
      "D132-H   Training Data Shape: (40000, 101)\n",
      "D132-H   Label Data Shape:    (40000,)\n"
     ]
    }
   ],
   "source": [
    "wildtype_data = np.loadtxt (\"thresh0.002_enriched_2-20aa_angle_result.txt\") # read in wildtype lcc data\n",
    "#wildtype_data = wildtype_data [:,1:] # delete first column which is frame number\n",
    "wildtype_label = np.zeros(len(wildtype_data)) # set wildtype labels to 0 \n",
    "\n",
    "mutant_1_data = np.loadtxt (\"thresh0.002_enriched_2-20aa_angle_result_D132H.txt\") # read in mutant lcc data\n",
    "#mutant_1_data = mutant_1_data [:,1:] # delete first column which is frame number\n",
    "mutant_1_label = np.ones(len(mutant_1_data)) # set mutant labels to 1\n",
    "\n",
    "print('Wildtype Training Data Shape:', wildtype_data.shape)\n",
    "print('Wildtype Label Data Shape:   ', wildtype_label.shape)\n",
    "print('D132-H   Training Data Shape:', mutant_1_data.shape)\n",
    "print('D132-H   Label Data Shape:   ', mutant_1_label.shape)\n",
    "\n",
    "for j in range(1000): # print out examples of random data sets\n",
    "    i = np.random.randint(0, len(wildtype_data)) # pick a random data set\n",
    "    plt.plot(wildtype_data[i], color = \"navy\", alpha = 0.002)\n",
    "    plt.plot(mutant_1_data[i], color = \"red\", alpha = 0.002)\n",
    "plt.savefig(\"input.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B0gsFQ9WKC-a",
    "outputId": "b08b8cb6-b06d-42e5-c916-7f23aa710f31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined input_data.shape: (80000, 101)\n",
      "Combined label_data.shape: (80000,) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generate combined and shuffled input data file\n",
    "lcc_data = np.vstack((wildtype_data, mutant_1_data))\n",
    "print (\"Combined input_data.shape:\", lcc_data.shape)\n",
    "\n",
    "label_data = np.hstack((wildtype_label, mutant_1_label))\n",
    "print (\"Combined label_data.shape:\", label_data.shape, \"\\n\")\n",
    "\n",
    "# here we shuffle both tensors simultaneously to maintain the labels with each data set\n",
    "lcc_data, label_data = unison_shuffled_copies (lcc_data, label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "DPl1VY9iKIRo",
    "outputId": "7eac15f3-6604-458b-cf83-611047fbec2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# check that the distribution is still the same after shuffling\n",
    "# this figure should look very similar to the one above\n",
    "label_summary = []\n",
    "test_1 = np.array (lcc_data)\n",
    "test_2 = np.array (label_data)\n",
    "for x in range (0, 1000):\n",
    "    if test_2[x] == 0.0: # identify label and color plot accordingly\n",
    "        plt.plot (test_1[x], color = \"navy\", alpha = 0.01);\n",
    "        label_summary.append(0)\n",
    "    if test_2[x] == 1.0:\n",
    "        plt.plot (test_1[x], color = \"magenta\", alpha = 0.01);\n",
    "        label_summary.append(1)\n",
    "plt.savefig(\"shuffled.png\", dpi=300)\n",
    "print (label_summary [0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RoC2Z5ZLbT-1",
    "outputId": "69a075dd-8a52-42a9-e2fe-25d41d3183a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of combined data points:\t\t\t 80000 \n",
      "Total number of data points selected for training:\t 64000 \n",
      "\n",
      "(64000, 101) (16000, 101)\n",
      "(64000,) (16000,)\n"
     ]
    }
   ],
   "source": [
    "#  normalize shuffled input data file\n",
    "upper_limit          = int(len (lcc_data)) # get total length of concatenated data\n",
    "upper_training_limit = int(len (lcc_data) * 0.8) # 80% of data used for training\n",
    "print (\"Total number of combined data points:\\t\\t\\t\",upper_limit, \"\\nTotal number of data points selected for training:\\t\", upper_training_limit, \"\\n\")\n",
    "\n",
    "lcc_data = lcc_data/180 # normalizing\n",
    "train_data = lcc_data [0:upper_training_limit,:] # select training data - first 80%\n",
    "test_data  = lcc_data [upper_training_limit:upper_limit,:] # select last 20% for testing\n",
    "\n",
    "train_label = label_data [0:upper_training_limit] # select truth data - first 80%\n",
    "test_label  = label_data [upper_training_limit:upper_limit] # select last 20% for testing\n",
    "\n",
    "print (train_data.shape, test_data.shape)\n",
    "print (train_label.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Martina's autoencoder\n",
    "\n",
    "LeReLU_alpha=0.2\n",
    "    \n",
    "#Input layer\n",
    "#input_data = tf.keras.layers.Input(shape=(190), name='ae_input')\n",
    "input_data = tf.keras.layers.Input(shape=(101))\n",
    "    \n",
    "encoder = Dense(256, activation=LeakyReLU(alpha=LeReLU_alpha), name='e1')(input_data)\n",
    "encoder = Dense(64,  activation=LeakyReLU(alpha=LeReLU_alpha), name='e2')(encoder)\n",
    "\n",
    "encoded = Dense(2,   activation=LeakyReLU(alpha=LeReLU_alpha), name='ae_latent')(encoder)\n",
    "    \n",
    "decoder = Dense(64,  activation=LeakyReLU(alpha=LeReLU_alpha), name='d1')(encoded)\n",
    "decoder = Dense(256, activation=LeakyReLU(alpha=LeReLU_alpha), name='d2')(decoder)\n",
    "\n",
    "output_layer = Dense(train_data.shape[1], activation=LeakyReLU(alpha=LeReLU_alpha), name='ae_output')(decoder)\n",
    "#decoded = tf.keras.layers.Dense(190)(decoder)\n",
    "\n",
    "autoencoder = tf.keras.models.Model(input_data, output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rHCXSIBkHxQE",
    "outputId": "6af0fec9-96d5-4560-c3f3-1d627c6fba15",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 101)]             0         \n",
      "_________________________________________________________________\n",
      "e1 (Dense)                   (None, 256)               26112     \n",
      "_________________________________________________________________\n",
      "e2 (Dense)                   (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "ae_latent (Dense)            (None, 2)                 130       \n",
      "_________________________________________________________________\n",
      "d1 (Dense)                   (None, 64)                192       \n",
      "_________________________________________________________________\n",
      "d2 (Dense)                   (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "ae_output (Dense)            (None, 101)               25957     \n",
      "=================================================================\n",
      "Total params: 85,479\n",
      "Trainable params: 85,479\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder = tf.keras.models.Model(inputs = input_data, outputs = output_layer)\n",
    "autoencoder.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=tf.keras.optimizers.Adam(learning_rate = 0.005))\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ln0ocLhVH5Gl",
    "outputId": "4e87328f-e8de-4b87-f1ea-145c57d2ba1b",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "plt.clf()\n",
    "for counts in range (0, 100): #this determines the number of epoch sets\n",
    "    history = autoencoder.fit(train_data, train_data, batch_size = 256, \n",
    "                              epochs = 1000, validation_data = (test_data, test_data))\n",
    "    # convert history object to dataframe and plot rates\n",
    "    training_history = pd.DataFrame(history.history)\n",
    "    plt.plot (training_history);\n",
    "    file_name_0 = \"history_\" + str(counts)\n",
    "    training_history.to_pickle(file_name_0)\n",
    "    file_name_1 = str(counts) + \".png\"\n",
    "    plt.savefig(file_name_1, dpi=300)\n",
    "    plt.clf()\n",
    "    \n",
    "    # read in latent layer\n",
    "    dr_model = tf.keras.models.Model(inputs  = autoencoder.get_layer('e1').input, \n",
    "                                     outputs = autoencoder.get_layer('ae_latent').output)\n",
    "    dr_model.summary()\n",
    "    \n",
    "    # put the validation data through current latent layer model\n",
    "    x = []\n",
    "    y = []\n",
    "    z = []\n",
    "    for i in range(8000):\n",
    "        z.append(test_label[i])\n",
    "        op = dr_model.predict(np.array([test_data[i]]))\n",
    "        x.append(op[0][0])\n",
    "        y.append(op[0][1])\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df['x'] = x\n",
    "    df['y'] = y\n",
    "    df['z'] = [\"trajectory-\" + str(k) for k in z]\n",
    " \n",
    "    plt.figure(figsize = (8, 6));\n",
    "    fig = sns.scatterplot(x = 'x', y='y', hue='z', data=df, s=10)\n",
    "    file_name_2 = str(counts) + \"_latent.png\"\n",
    "    fig.figure.savefig(file_name_2, dpi = 300)\n",
    "    plt.clf()\n",
    "    \n",
    "    #\n",
    "    df.to_pickle(str(counts))\n",
    "    file_name = 'models/saved_model_' + str(counts)\n",
    "    autoencoder.save(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2_wt-D132-H_trajectory_autoencoder.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
